@INPROCEEDINGS{Bluhm:Sonar:2009,
  author = {Andreas Bluhm and Jens Eickmeyer and Tobias Feith and Nikita Mattar
	and Thies Pfeiffer},
  title = {{E}xploration von sozialen {N}etzwerken im {3D} {R}aum am {B}eispiel
	von $\mbox{SoN}\forall \mbox{R\ }$ f\"ur {L}ast.fm},
  booktitle = {Virtuelle und Erweiterte Realit\"at - Sechster Workshop der GI-Fachgruppe
	VR/AR},
  year = {2009},
  editor = {Andreas Gerndt and Marc Erich Latoschik},
  pages = {269--280},
  address = {Aachen},
  publisher = {Shaker Verlag}
}

@article{1,
  abstract     = {Industrie  4.0  (English  translation:  Industry  4.0) stands for functional integration, dynamic reorganization, and resource  efficiency.  Technical  advances  in  control  and  communication create infrastructures that handle more and more tasks automatically. As a result, the complexity of today’s and future  technical  systems  is  hidden  from  the  user.  These  advances, however, come with distinct challenges for user interface design.  A central  question is: how to  empower  users  to understand, monitor, and control the automated processes of Industrie 4.0? Addressing these design challenges requires a full  integration  of user-centered  design  (UCD) processes  into the  development  process.  This  paper  discusses  flexible  but powerful methods for usability and user experience engineering in the context of Industrie 4.0.},
  author       = {Pfeiffer, Thies and Hellmers, Jens and Schön, Eva-Maria and Thomaschewski, Jörg},
  journal      = {IEEE Proceedings Special Issue on Cyberphysical Systems},
  keyword      = {user centered design, industry 4.0, eye tracking, agile},
  number       = {5},
  pages        = {986 -- 996},
  publisher    = {IEEE},
  title        = {{Empowering User Interfaces for the Industry 4.0}},
  doi          = {10.1109/JPROC.2015.2508640  },
  volume       = {104},
  year         = {2016},
}

@inproceedings{2,
  abstract     = {While display quality and rendering for Head-Mounted-Displays (HMDs) has increased in quality and performance, the interaction capabilities with these devices are still very limited or relying on expensive technology. Current experiences offered for mobile HMDs often stick to dome-like looking around, automatic or gaze-triggered movement, or flying techniques. 

We developed an easy to use walking-in-place technique that does not require additional hardware to enable basic navigation, such as walking, running, or jumping, in virtual environments. Our approach is based on the analysis of data from the inertial unit embedded in mobile HMDs. In a first prototype realized for the Samsung Galaxy Gear VR we detect steps and jumps. A user study shows that users novice to virtual reality easily pick up the method. In comparison to a classic input device, using our walking-in-place technique study participants felt more present in the virtual environment and preferred our method for exploration of the virtual world. },
  author       = {Pfeiffer, Thies and Schmidt, Aljoscha and Renner, Patrick},
  booktitle    = {IEEE Virtual Reality 2016},
  keyword      = {Virtual Reality, Ultra Mobile Head Mounted Displays, Navigation, Walking-in-place},
  publisher    = {IEEE},
  title        = {{Detecting Movement Patterns from Inertial Data of a Mobile Head-Mounted-Display for Navigation via Walking-in-Place}},
  year         = {2016},
}

@inproceedings{3,
  abstract     = {With the launch of ultra-portable systems, mobile eye tracking finally has the potential to become mainstream. While eye movements on their own can already be used to identify human activities, such as reading or walking, linking eye movements to objects in the environment provides even deeper insights into human cognitive processing.

We present a model-based approach for the identification of fixated objects in three-dimensional environments. For evaluation, we compare the automatic labelling of fixations with those performed by human annotators. In addition to that, we show how the approach can be extended to support moving targets, such as individual limbs or faces of human interaction partners. The approach also scales to studies using multiple mobile eye-tracking systems in parallel.

The developed system supports real-time attentive systems that make use of eye tracking as means for indirect or direct human-computer interaction as well as off-line analysis for basic research purposes and usability studies. },
  author       = {Pfeiffer, Thies and Renner, Patrick and Pfeiffer-Leßmann, Nadine},
  booktitle    = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \& Applications},
  keyword      = {Eyetracking, Gaze-based Interaction},
  location     = {Charleston, SC, USA},
  pages        = {189--196},
  publisher    = {ACM Press},
  title        = {{EyeSee3D 2.0: Model-based Real-time Analysis of Mobile Eye-Tracking in Static and Dynamic Three-Dimensional Scenes}},
  doi          = {10.1145/2857491.2857532},
  year         = {2016},
}

@inproceedings{4,
  abstract     = {Heat maps, or more generally, attention maps or saliency maps are an often used technique to visualize eye-tracking data. With heat maps qualitative information about visual processing can be easily visualized and communicated between experts and laymen. They are thus a versatile tool for many disciplines, in particular for usability engineering, and are often used to get a first overview about recorded eye-tracking data.

Today, heat maps are typically generated for 2D stimuli that have been presented on a computer display. In such cases the mapping of overt visual attention on the stimulus is rather straight forward and the process is well understood. However, when turning towards mobile eye tracking and eye tracking in 3D virtual environments, the case is much more complicated. 

In the first part of the paper, we discuss several challenges that have to be considered in 3D environments, such as changing perspectives, multiple viewers, object occlusions, depth of fixations, or dynamically moving objects. In the second part, we present an approach for the generation of 3D heat maps addressing the above mentioned issues while working in real-time. Our visualizations provide high-quality output for multi-perspective eye-tracking recordings of visual attention in 3D environments.},
  author       = {Pfeiffer, Thies and Memili, Cem},
  booktitle    = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \& Applications},
  keyword      = {Eyetracking, Gaze-based Interaction},
  location     = {Charleston, SC, USA},
  pages        = {95--102},
  publisher    = {ACM Press},
  title        = {{Model-based Real-time Visualization of Realistic Three-Dimensional Heat Maps for Mobile Eye Tracking and Eye Tracking in Virtual Reality}},
  doi          = {10.1145/2857491.2857541},
  year         = {2016},
}

@inproceedings{5,
  abstract     = {Virtual Reality (VR) has the potential to support motor learning in ways exceeding beyond the possibilities provided by real world environments. New feedback mechanisms can be implemented that support motor learning during the performance of the trainee and afterwards as a performance review. As a consequence, VR environments excel in controlled evaluations, which has been proven in many other application scenarios.

However, in the context of motor learning of complex tasks, including full-body movements, questions regarding the main technical parameters of such a system, in particular that of the required maximum latency, have not been addressed in depth. To fill this gap, we propose a set of requirements towards VR systems for motor learning, with a special focus on motion capturing and rendering. We then assess and evaluate state-of-the-art techniques and technologies for motion capturing and rendering, in order to provide data on latencies for different setups. We focus on the end-to-end latency of the overall system, and present an evaluation of an exemplary system that has been developed to meet these requirements.
},
  author       = {Waltemate, Thomas and Hülsmann, Felix and Pfeiffer, Thies and Kopp, Stefan and Botsch, Mario},
  booktitle    = {Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology},
  keyword      = {low-latency, motor learning, virtual reality},
  location     = {Beijing, China},
  pages        = {139--147},
  publisher    = {ACM},
  title        = {{Realizing a Low-latency Virtual Reality Environment for Motor Learning}},
  doi          = {10.1145/2821592.2821607},
  year         = {2015},
}

@inproceedings{6,
  abstract     = {Visual attention can be a viable source of information to assess human behaviors in many different contexts, from human-computer interaction, over sports or social interactions, to complex working environments, such as to be found in the context of Industry 4.0. In such scenarios in which the user is able to walk around freely, mobile eye-tracking systems are used to record eye movements, which are then mapped onto an ego-perspective video. The analysis of such recordings then requires large efforts for manually annotating the recorded videos on a frame-by-frame basis to label the fixations based on their locations to the target objects present in the video. First, we present a method to record eye movements in 3D scenarios and annotate fixations with corresponding labels for the objects of interest in real-time 2. For this purpose, we rely on computer-vision methods for the detection of the camera position and orientation in the world. Based on a coarse 3D model of the environment, representing the 3D areas of interest, fixations are mapped to areas of interest. As a result, we can identify the position of the fixation in terms of local object coordinates for each relevant object of interest. Second, we present a method for real-time creation and visualization of heatmaps for 3D objects 1. Based on a live-streaming of the recorded and analyzed eye movements, our solution renders heatmaps on top of the object s urfaces. The resulting visualizations are more realistic than standard 2D heatmaps, in that we consider occlusions, depth of focus and dynamic moving objects. Third, we present a new method which allows us to aggregate fixations on a per object basis, e.g. similar to regions/areas of interest. This allows us to transfer existing methods of analysis to 3D environments. We present examples from a virtual supermarket, a study on social interactions between two humans, examples from real-time gaze mapping on body parts of a moving humans and from studying 3D prototypes in a virtual reality environment.},
  author       = {Pfeiffer, Thies and Memili, Cem and Renner, Patrick},
  booktitle    = {Proceedings of the 2nd International Workshop on Solutions for Automatic Gaze Data Analysis 2015 (SAGA 2015)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking},
  location     = {Bielefeld},
  pages        = {11--12},
  publisher    = {eCollections Bielefeld University},
  title        = {{Capturing and Visualizing Eye Movements in 3D Environments}},
  doi          = {10.2390/biecoll-saga2015_3},
  year         = {2015},
}

@misc{7,
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking},
  location     = {Bielefeld},
  publisher    = {eCollections Bielefeld University},
  title        = {{Proceedings of the 2nd International Workshop on Solutions for Automatic Gaze Data Analysis 2015 (SAGA 2015)}},
  doi          = {10.2390/biecoll-saga2015_21},
  year         = {2015},
}

@inproceedings{8,
  abstract     = {Every now and then there are situations in which we are not sure how to proceed and thus are seeking for help. For example, choosing the best product out of dozens of different brands in a supermarket can be difficult, especially when following a specific diet. There are, however, also people who have problems with decision making or sequencing actions in everyday life, e.g. because they suffer from dementia. In such situations, it may be welcomed when there is someone around noticing our problem and offering help. In more private situations, e.g. in the bathroom, help in shape of a human being cannot be expected or even is not welcomed. Our research focuses on the design of mobile assistive systems which could assist in everyday life activities. Such a system needs to detect situations of helplessness, identify the interaction context, conclude what would be an appropriate assistance, before finally engaging in interaction with the user.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {SAGA 2015: 2nd International Workshop on Solutions for Automatic Gaze Data Analysis},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking, gaze-based interaction, ADAMAAS},
  location     = {Bielefeld},
  pages        = {14--15},
  publisher    = {eCollections Bielefeld University},
  title        = {{Online Visual Attention Monitoring for Mobile Assistive Systems}},
  doi          = {10.2390/biecoll-saga2015_6},
  year         = {2015},
}

@inproceedings{9,
  abstract     = {Eye gaze plays an important role in human communication. One foundational skill in human social interaction is joint attention which is receiving increased interest in particular in the area of human-agent or human-robot interaction. We are focusing here on patterns of gaze interaction that emerge in the process of establishing joint attention. The approach, however, should be applicable to many other aspects of social communication in which eye gaze plays an important role. Attention has been characterized as an increased awareness 1 and intentionally directed perception 2 and is judged to be crucial for goal-directed behavior. Joint attention can be defined as simultaneously allocating attention to a target as a consequence of attending to each other's attentional states 3. In other words: Interlocutors have to deliberatively focus on the same target while being mutually aware of sharing their focus of attention 2 4.},
  author       = {Pfeiffer-Leßmann, Nadine and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {SAGA 2015: 2nd International Workshop on Solutions for Automatic Gaze Data Analysis},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {joint attention, eye tracking},
  location     = {Bielefeld},
  pages        = {27--29},
  publisher    = {eCollections Bielefeld University},
  title        = {{Analyzing Patterns of Eye Movements in Social Interactions}},
  doi          = {10.2390/biecoll-saga2015_10},
  year         = {2015},
}

@article{10,
  abstract     = {Our research aims at cognitive modelling of joint attention for artificial agents, such as virtual
agents or robots. With the current study, we are focusing on the observation of interaction
patterns of joint attention and their time course. For this, we recorded and analyzed
twenty sessions of two interacting participants using two mobile binocular eye-tracking
systems.
A key contribution of our work addresses methodological aspects of mobile eye-tracking
studies with scene camera recordings in general. The standard procedure for the analysis of
such gaze videos requires a manual annotation. This time consuming process often exceeds
multiple times the duration of the original recordings (e.g. 30 times). This doubles if, as in
our case, the gaze of two interlocutors is recorded simultaneously.
In our approach, we build upon our EyeSee3D approach for marker-based tracking and
registration of the environment and a 3D reconstruction of the relevant stimuli. We extend
upon the previous approach in supporting more than one participant and dynamically
changing stimuli, here the faces and eyes of the interlocutors. The full analysis of the time
course of both interlocutor’s gaze is done in real-time and available for analysis right after
the experiment without the requirement for manual annotation.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Pfeiffer-Leßmann, Nadine},
  journal      = {Abstracts of the 18th European Conference on Eye Movements},
  keyword      = {eye tracking, joint attention, 3D},
  pages        = {116--116},
  title        = {{Automatic Analysis of a Mobile Dual Eye-Tracking Study on Joint Attention}},
  year         = {2015},
}

@article{11,
  abstract     = {Current semantic theory on indexical expressions claims that demonstratively used indexicals such as this lack a referent-determining meaning but instead rely on an accompanying demonstration act like a pointing gesture. While this view allows to set up a sound logic of demonstratives, the direct-referential role assigned to pointing gestures has never been scrutinized thoroughly in semantics or pragmatics. We investigate the semantics and pragmatics of co-verbal pointing from a foundational perspective combining experiments, statistical investigation, computer simulation and theoretical modeling techniques in a novel manner. We evaluate various referential hypotheses with a corpus of object identification games set up in experiments in which body movement tracking techniques have been extensively used to generate precise pointing measurements. Statistical investigation and computer simulations show that especially distal areas in the pointing domain falsify the semantic direct-referential hypotheses concerning pointing gestures. As an alternative, we propose that reference involving pointing rests on a default inference which we specify using the empirical data. These results raise numerous problems for classical semantics–pragmatics interfaces: we argue for pre-semantic pragmatics in order to account for inferential reference in addition to classical post-semantic Gricean pragmatics.},
  author       = {Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes},
  journal      = {Journal of Pragmatics},
  keyword      = {Multimodal Communication
Gestures
Pointing
Reference},
  pages        = {56--79},
  publisher    = {Elsevier},
  title        = {{Pointing and reference reconsidered}},
  doi          = {10.1016/j.pragma.2014.12.013},
  volume       = {77},
  year         = {2015},
}

@article{12,
  abstract     = {Eye tracking helps evaluate the quality of data visualization techniques and facilitates advanced interaction techniques for visualization
systems.},
  author       = {Kurzhals, Kuno and Burch, Michael and Pfeiffer, Thies and Weiskopf, Daniel},
  journal      = {Computing in Science \& Engineering},
  keyword      = {Eyetracking},
  number       = {5},
  pages        = {64--71},
  publisher    = {IEEE},
  title        = {{Eye Tracking in Computer-Based Visualization}},
  doi          = {10.1109/MCSE.2015.93},
  volume       = {17},
  year         = {2015},
}

@inproceedings{13,
  abstract     = {Die Erkennung und Verfolgung von Objekten ist seit Jahrzehnten eine wichtige Basis für Anwendungen im Bereich der Erweiterten Realität. Muss aus der Vielzahl an Bibliotheken eine verfügbare ausgewählt werden, fehlt es häufig an Vergleichbarkeit, da es kein standardisiertes Testverfahren gibt. Gleichzeitig ist es für die Entwicklung
eigener Verfahren essentiell, das Optimierungspotential genau bestimmen zu können. Die vorliegende Arbeit versucht diese Lücke zu füllen: Mithilfe von systematisch erstellten gerenderten Videos können verschiedene Verfahren und Bibliotheken bezüglich ihrer Genauigkeit
und Performanz überprüft werden. Exemplarisch werden die Eigenschaften dreier Trackingbibliotheken miteinander verglichen.},
  author       = {Diekmann, Jonas and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 12. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Hinkenjann, André and Maiero, Jens and Blach, Roland},
  location     = {Bonn},
  pages        = {89--100},
  publisher    = {Shaker Verlag},
  title        = {{Framework zur Evaluation von Trackingbibliotheken mittels gerenderter Videos von Tracking-Targets}},
  year         = {2015},
}

@inproceedings{14,
  abstract     = {In diesem Paper wird die Entwicklung und Evaluation eines kostengünstigen interaktiven Aufprojektionssystems auf Basis einer Microsoft Kinect 2 beschrieben. Das Aufprojektionssystem nutzt einen LED-Projektor zur Darstellung von Informationen auf einer ebenen Projektionsfläche. Durch eine Analyse von Infrarot- und Tiefenbild werden Fingerbewegungen erkannt und als Multi-Touch-Events auf Windows-Betriebssystemebene bereitgestellt.
Die Tragfähigkeit des Ansatzes wird bezogen auf die erreichbare Genaugkeit und die Nutzbarkeit in einer Nutzerstudie evaluiert. Abschließend werden Designempfehlungen für die Gestaltung von Benutzerschnittstellen mit dem entwickelten interaktiven Aufprojektionssystem formuliert.},
  author       = {Neumann, Henri  and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 12. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Hinkenjann, André and Maiero, Jens and Blach, Roland},
  location     = {Bonn},
  pages        = {22--33},
  publisher    = {Shaker Verlag},
  title        = {{Entwicklung und Evaluation eines Kinect v2-basierten Low-Cost Aufprojektionssystems}},
  year         = {2015},
}

@inproceedings{15,
  abstract     = {Measuring visual attention has become an important tool during product development. Attention maps are important qualitative visualizations to communicate results within the team and to stakeholders. We have developed a GPU-accelerated approach which allows for real-time generation of attention maps for 3D models that can, e.g., be used for on-the-fly visualizations of visual attention
distributions and for the generation of heat-map textures for offline high-quality renderings. The presented approach is unique in that it works with monocular and binocular data, respects the depth of focus, can handle moving objects and is ready to be used for selective
rendering.},
  author       = {Pfeiffer, Thies and Memili, Cem},
  booktitle    = {Proceedings of the IEEE VR 2015},
  editor       = {Höllerer, Tobias and Interrante, Victoria and Lécuyer, Anatole and II, J. Edward Swan},
  keyword      = {Attention Volumes, Gaze-based Interaction, 3D, Eye Tracking},
  pages        = {257--258},
  publisher    = {IEEE},
  title        = {{GPU-accelerated Attention Map Generation for Dynamic 3D Scenes}},
  year         = {2015},
}

@inbook{16,
  abstract     = {We present research-in-progress on an attentive in-store mobile recommender system that is integrated into the user’s glasses and worn during purchase decisions. The system makes use of the Attentive Mobile Interactive Cognitive Assistant (AMICA) platform prototype designed as a ubiquitous technology that supports people in their everyday-life. This paper gives a short overview of the technology and presents results from a pre-study in which we collected real-life eye-tracking data during decision processes in a supermarket. The data helps us to characterize and identify the different decision contexts based on differences in the observed attentional processes. AMICA provides eye-tracking data that can be used to classify decision-making behavior in real-time to make a recommendation process context-aware.},
  author       = {Pfeiffer, Jella and Pfeiffer, Thies and Meißner, Martin},
  booktitle    = {Reshaping Society through Analytics, Collaboration, and Decision Support},
  editor       = {Power, Daniel and Lakshmi, Iyer},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  pages        = {161--173},
  publisher    = {Springer International Publishing},
  title        = {{Towards attentive in-store recommender systems}},
  volume       = {18},
  year         = {2015},
}

@article{17,
  abstract     = {Human-robot interaction in shared spaces might benefit from human skills of anticipating movements. We observed human-human interactions in a route planning scenario to   identify relevant communication strategies with a focus on hand-eye coordination.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Wachsmuth, Sven},
  journal      = {Cognitive Processing},
  keyword      = {gestures
eye tracking
robotics
},
  number       = {1 Supplement},
  pages        = {59--60},
  title        = {{Towards a model for anticipating human gestures in human-robot interactions in shared space}},
  volume       = {15},
  year         = {2014},
}

@article{18,
  abstract     = {We  present  an approach to identify  the 3D point  of regard and the fixated object in real-time based on 2D gaze videos without  the  need for manual annotation. The approach does not require  additional hardware except for the mobile eye tracker. It is currently applicable for scenarios with static target objects and requires an instrumentation of the environment with markers. The system has already been tested in two different studies. Possible applications are visual world paradigms in complex 3D environments, research on visual attention or human-human/human-agent interaction studies.},
  author       = {Pfeiffer, Thies and Renner, Patrick and Pfeiffer-Leßmann, Nadine},
  journal      = {Cognitive Processing},
  keyword      = {Gaze-based Interaction
Eye Tracking},
  number       = {Suppl. 1},
  pages        = {S127--S129},
  publisher    = {Springer},
  title        = {{Efficient analysis of gaze-behavior in 3D environments}},
  volume       = {15},
  year         = {2014},
}

@inbook{19,
  abstract     = {For solving tasks cooperatively in close interaction with humans, robots need to have timely updated spatial representations. However, perceptual information about the current position of interaction partners is often late. If robots could anticipate the targets of upcoming manual actions, such as pointing gestures, they would have more time to physically react to human movements and could consider prospective space allocations in their planning. Many findings support a close eye-hand coordination in humans which could be used to predict gestures by observing eye gaze. However, effects vary strongly with the context of the interaction. We collect evidence of eye-hand coordination in a natural route planning scenario in which two agents interact over a map on a table. In particular, we are interested if fixations can predict pointing targets and how target distances affect the interlocutor's pointing behavior. We present an automatic method combining marker tracking and 3D modeling that provides eye and gesture measurements in real-time.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Spatial Cognition IX},
  editor       = {Freksa, Christian and Nebel, Bernhard and Hegarty, Mary and Barkowsky, Thomas},
  keyword      = {gestures, robotics, eye tracking, multimodal interaction},
  pages        = {121--136},
  publisher    = {Springer},
  title        = {{Spatial references with gaze and pointing in shared space of humans and robots}},
  doi          = {10.1007/978-3-319-11215-2_9},
  volume       = {8684},
  year         = {2014},
}

@inproceedings{20,
  abstract     = {For solving complex tasks cooperatively in close interaction with robots, they need to understand natural human communication. To achieve this, robots could benefit from a deeper understanding of the processes that humans use for successful communication. Such skills can be studied by investigating human face-to-face interactions in complex tasks. In our work the focus lies on shared-space interactions in a path planning task and thus 3D gaze directions and hand movements are of particular interest. However, the analysis of gaze and gestures is a time-consuming task: Usually, manual annotation of the eye tracker's scene camera video is necessary in a frame-by-frame manner. To tackle this issue, based on the EyeSee3D method, an automatic approach for annotating interactions is presented: A combination of geometric modeling and 3D marker tracking serves to align real world stimuli with virtual proxies. This is done based on the scene camera images of the mobile eye tracker alone. In addition to the EyeSee3D approach, face detection is used to automatically detect fixations on the interlocutor. For the acquisition of the gestures, an optical marker tracking system is integrated and fused in the multimodal representation of the communicative situation.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  keyword      = {Eyetracking, geometric modelling, motion tracking, Gaze-based Interaction, 3D gaze analysis, Augmented Reality, eye tracking, marker tracking},
  pages        = {361--362},
  publisher    = {ACM},
  title        = {{Model-based acquisition and analysis of multimodal interactions for improving human-robot interaction}},
  doi          = {10.1145/2578153.2582176},
  year         = {2014},
}

@inproceedings{21,
  abstract     = {For validly analyzing human visual attention, it is often necessary to proceed from computer-based desktop set-ups to more natural real-world settings. However, the resulting loss of control has to be counterbalanced by increasing 
participant and/or item count. Together with the effort required to manually annotate the gaze-cursor videos recorded with mobile eye trackers, this renders many studies unfeasible.

We tackle this issue by minimizing the need for manual annotation of mobile gaze data. Our approach combines geo\-metric modelling with inexpensive 3D marker tracking to align virtual proxies with the real-world objects. This allows us to classify fixations on objects of interest automatically while supporting a completely free moving participant.

The paper presents the EyeSee3D method as well as a comparison of an expensive outside-in (external cameras) and a low-cost inside-out (scene camera) tracking of the eyetracker's position. The EyeSee3D approach is evaluated comparing the results from automatic and manual classification of fixation targets, which raises old problems of annotation validity in a modern context.},
  author       = {Pfeiffer, Thies and Renner, Patrick},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  keyword      = {Gaze-based Interaction
Eyetracking
Augmented Reality
},
  pages        = {195--202},
  publisher    = {ACM},
  title        = {{EyeSee3D: a low-cost approach for analysing mobile 3D eye tracking data using augmented reality technology}},
  doi          = {10.1145/2578153.2578183},
  year         = {2014},
}

@inproceedings{22,
  abstract     = {In this paper, we investigate the visual attention of consumers with the help of mobile eye-tracking technology. We explore attentional differences between goal-directed search and exploratory search used when consumers are purchasing a product at the point-of-sale. The aim of this study is to classify these two search processes based solely on the consumersâ eye movements. Using data from a field experiment in a supermarket, we build a model that learns about consumersâ attentional processes and makes predictions about the search process they used. Our results show that we can correctly classify the search processes used with an accuracy of nearly 70 after just the first nine seconds of the search. Later on in the search process, the accuracy of the classification can reach up to 77.},
  author       = {Pfeiffer, Jella and Meißner, Martin and Prosiegel, Jascha and Pfeiffer, Thies},
  booktitle    = {Proceedings of the International Conference on Information Systems 2014 (ICIS 2014)},
  keyword      = {Recommendation Systems, Decision Support Systems (DSS), Human Information Behavior, Neuro-IS, Mobile commerce},
  title        = {{Classification of Goal-Directed Search and Exploratory Search Using Mobile Eye-Tracking}},
  year         = {2014},
}

@inproceedings{23,
  abstract     = {Previous work on eye tracking and eye-based
human-computer interfaces mainly concentrated on
making use of the eyes in traditional desktop settings.
With the recent growth of interest in smart glass devices
and low-cost eye trackers, however, gaze-based techniques
for mobile computing is becoming increasingly important.
PETMEI 2014 focuses on the pervasive eye tracking
paradigm as a trailblazer for mobile eye-based interaction
and eye-based context-awareness. We want to stimulate
and explore the creativity of these communities with
respect to the implications, key research challenges, and
new applications for pervasive eye tracking in ubiquitous
computing. The long-term goal is to create a strong
interdisciplinary research community linking these fields
together and to establish the workshop as the premier
forum for research on pervasive eye tracking.},
  author       = {Pfeiffer, Thies and Stellmach, Sophie and Sugano, Yusuke},
  booktitle    = {UbiComp'14 Adjunct: The 2014 ACM Conference on Ubiquitous Computing Adjunct Publication},
  keyword      = {Gaze-based Interaction},
  pages        = {1085--1092},
  title        = {{4th International Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction}},
  year         = {2014},
}

@inproceedings{24,
  abstract     = {In many applications it is necessary to guide humans' visual attention towards certain points in the environment. This can be to highlight certain attractions in a touristic application for smart glasses, to signal important events to the driver of a car or to draw the attention of a user of a desktop system to an important message of the user interface. The question we are addressing here is: How can we guide visual attention if we are not able to do it visually? In the presented approach we use gaze-contingent auditory feedback (sonification) to guide visual attention and show that people are able to make use of this guidance to speed up visual search tasks significantly.},
  author       = {Losing, Viktor and Rottkamp, Lukas and Zeunert, Michael and Pfeiffer, Thies},
  booktitle    = {UbiComp'14 Adjunct: The 2014 ACM Conference on Ubiquitous Computing Adjunct Publication},
  keyword      = {Gaze-based Interaction, Visual Search},
  location     = {Seattle, WA, USA},
  pages        = {1093--1102},
  publisher    = {ACM Press},
  title        = {{Guiding Visual Search Tasks Using Gaze-Contingent Auditory Feedback}},
  year         = {2014},
}

@inproceedings{25,
  abstract     = {Since the widespread diffusion of mobile devices, like smartphones, mobile decision support systems (MDSS) that provide product information, recommendations or other kind of decision support for in-store purchases have gained momentum. A user-centered design of MDSS requires a choice of features appropriate for the specific decision situation. This paper presents results of a study to identify important features customers expect of an in-store MDSS for electronic devices in different purchase situations. The study has been conducted as online questionnaire applying a preference measurement technique from marketing science.},
  author       = {Huschens, Martin and Pfeiffer, Jella and Pfeiffer, Thies},
  booktitle    = {Proceedings of the MKWI},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  title        = {{Important product features of mobile decision support systems for in-store purchase decisions: A user-perspective taking into account different purchase situations}},
  year         = {2014},
}

@inproceedings{26,
  abstract     = {One of the latest trends of ubiquitous Information Systems is the use of smartglasses, such as Google Glass or Epson Moverio BT-200 that are connected to the Internet and are augmenting reality with a head-up display. In order to develop recommendation agents (RAs) for the use at the point of sale, researchers have proposed to integrate a portable eye tracking system into such smartglasses (Pfeiffer et al. 2013). This would allow providing the customer with relevant product information and alternative products by making use of the customer’s information acquisition processes recorded during the purchase decision.},
  author       = {Pfeiffer, Jella and Prosiegel, Jascha and Meißner, Martin and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Gmunden Retreat on NeuroIS 2014},
  editor       = {Davis, Fred and Riedl, René and vom Brocke, Jan and Léger, Pierre-Majorique and Randolph, Adriane},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  location     = {Gmunden, Austria},
  pages        = {23--25},
  title        = {{Identifying goal-oriented and explorative information search patterns}},
  year         = {2014},
}

@inproceedings{27,
  author       = {Kousidis, Spyridon and Pfeiffer, Thies and Schlangen, David},
  booktitle    = {Proceedings of Interspeech 2013},
  keyword      = {Multimodal Communication},
  location     = {Lyon, France},
  publisher    = {ISCA},
  title        = {{MINT.tools: Tools and Adaptors Supporting Acquisition, Annotation and Analysis of Multimodal Corpora}},
  year         = {2013},
}

@inbook{28,
  abstract     = {The eyes play an important role both in perception and communication. Technical interfaces that make use of their versatility can bring significant improvements to those who are unable to speak or to handle selection tasks elsewise such as with their hands, feet, noses or tools handled with the mouth. Using the eyes to enter texts into a computer system, which is called gaze-typing, is the most prominent gaze-based assistive technology. The article reviews the principles of eye movements, presents an overview of current eye-tracking systems, and discusses several approaches to gaze-typing. With the recent advent of mobile eye-tracking systems, gaze-based assistive technology is no longer restricted to interactions with desktop-computers. Gaze-based assistive technology is ready to expand its application into other areas of everyday life. The second part of the article thus discusses the use of gaze-based assistive technology in the household, or “the wild,” outside one’s own four walls.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Assistive Technologies and Computer Access for Motor Disabilities},
  editor       = {Kouroupetroglou, Georgios},
  keyword      = {Gaze-based Interaction
Eye Tracking},
  pages        = {90--109},
  publisher    = {IGI Global},
  title        = {{Gaze-based assistive technologies}},
  doi          = {10.4018/978-1-4666-4438-0.ch004},
  year         = {2013},
}

@misc{29,
  author       = {Pfeiffer, Thies and Essig, Kai},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  editor       = {Holmqvist, Kenneth and Mulvey, F. and Johansson, Roger},
  keyword      = {Gaze-based Interaction, Eyetracking},
  location     = {Lund, Sweden},
  number       = {3},
  pages        = {275--275},
  publisher    = {Journal of Eye Movement Research},
  title        = {{Analysis of eye movements in situated natural interactions}},
  volume       = {6},
  year         = {2013},
}

@inproceedings{30,
  abstract     = {We aim to utilize online information about visual attention for developing mobile recommendation agents (RAs) for use at the point of sale. Up to now, most RAs are focussed exclusively at personalization in an e-commerce setting. Very little is known, however, about mobile RAs that offer information and assistance at the point of sale based on individual-level feature based preference models (Murray and Häubl 2009). Current attempts provide information about products at the point of sale by manually scanning barcodes or using RFID (Kowatsch et al. 2011, Heijden 2005), e.g. using specific apps for smartphones. We argue that an online access to the current visual attention of the user offers a much larger potential. Integrating mobile eye tracking into ordinary glasses would yield a direct benefit of applying neuroscience methods in the user’s everyday life. First, learning from consumers’ attentional processes over time and adapting recommendations based on this learning allows us to provide very accurate and relevant recommendations, potentially increasing the perceived usefulness. Second, our proposed system needs little explicit user input (no scanning or navigation on screen) making it easy to use. Thus, instead of learning from click behaviour and past customer ratings, as it is the case in the e-commerce setting, the mobile RA learns from eye movements by participating online in every day decision processes. We argue that mobile RAs should be built based on current research in human judgment and decision making (Murray et al. 2010). In our project, we therefore follow a two-step approach: In the empirical basic research stream, we aim to understand the user’s interaction with the product shelf: the actions and patterns of user’s behaviour (eye movements, gestures, approaching a product closer) and their correspondence to the user’s informational needs. In the empirical system development stream, we create prototypes of mobile RAs and test experimentally the factors that influence the user’s adoption. For example, we suggest that a user’s involvement in the process, such as a need for exact nutritional information or for assistance (e.g., reading support for elderly) will influence the user’s intention to use such as system. The experiments are conducted both in our immersive virtual reality supermarket presented in a CAVE, where we can also easily display information to the user and track the eye movement in great accuracy, as well as in real-world supermarkets (see Figure 1), so that the findings can be better generalized to natural decision situations (Gidlöf et al. 2013). In a first pilot study with five randomly chosen participants in a supermarket, we evaluated which sort of mobile RAs consumers favour in order to get a first impression of the user’s acceptance of the technology. Figure 1 shows an excerpt of one consumer’s eye movements during a decision process. First results show long eye cascades and short fixations on many products in situations where users are uncertain and in need for support. Furthermore, we find a surprising acceptance of the technology itself throughout all ages (23 – 61 years). At the same time, consumers express serious fear of being manipulated by such a technology. For that reason, they strongly prefer the information to be provided by trusted third party or shared with family members and friends (see also Murray and Häubl 2009). Our pilot will be followed by a larger field experiment in March in order to learn more about factors that influence the user’s acceptance as well as the eye movement patterns that reflect typical phases of decision processes and indicate the need for support by a RA.},
  author       = {Pfeiffer, Thies and Pfeiffer, Jella and Meißner, Martin},
  booktitle    = {Proceedings of the Gmunden Retreat on NeuroIS 2013},
  editor       = {Davis, Fred and Riedl, René and Jan, vom Brocke and Léger, Pierre-Majorique and Randolph, Adriane},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  location     = {Gmunden},
  pages        = {3--3},
  title        = {{Mobile recommendation agents making online use of visual attention information at the point of sale}},
  year         = {2013},
}

@misc{31,
  author       = {Pfeiffer, Thies and Essig, Kai},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  pages        = {275},
  publisher    = {Journal of Eye Movement Research},
  title        = {{Analysis of eye movements in situated natural interactions}},
  year         = {2013},
}

@inproceedings{32,
  abstract     = {The provision of stereo images to facilitate depth perception by stereopsis is one key aspect of many Virtual Reality installations and there are many technical approaches to do so. However, differences in visual capabilities of the user and technical limitations of a specific set-up might restrict the spatial range in which stereopsis can be facilitated. In this paper, we transfer an existent test for stereo vision from the real world to a virtual environment and extend it to measure stereo acuity.},
  author       = {Dankert, Timo and Heil, Dimitri and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 10. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Staadt, Oliver and Steinicke, Frank},
  keyword      = {Virtual Reality},
  pages        = {185--188},
  publisher    = {Shaker Verlag},
  title        = {{Stereo vision and acuity tests within a virtual reality set-up}},
  year         = {2013},
}

@misc{33,
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction},
  location     = {Bielefeld},
  publisher    = {CITEC},
  title        = {{Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)}},
  doi          = {10.2390/biecoll-saga2013_0},
  year         = {2013},
}

@inproceedings{34,
  abstract     = {Modern mobile eye trackers calculate the point-of-regard relatively to the current image obtained by a scene-camera. They show where the wearer of the eye tracker is looking at in this 2D picture, but they fail to provide a link to the object of interest in the environment. To understand the context of the wearer’s current actions, human annotators therefore have to label the recorded fixations manually. This is very time consuming and also prevents an online interactive use in HCI. A popular scenario for mobile eye tracking are supermarkets. Gidlöf et al. (2013) used this scenario to study the visual behaviour in a decision-process. De Beugher et al. (2012) developed an offline approach to automate the analysis of object identification. For usage of mobile eye tracking in an online recommender system (Pfeiffer et al., 2013), that supports the user in a supermarket, it is essential to identify the object of interest immediately. Our work addresses this issue by using location information to speed-up the identification of the fixated object and at the same time making detection results more robust.},
  author       = {Harmening, Kai and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction
Mobile Cognitive Assistance Systems},
  location     = {Bielefeld},
  pages        = {38--40},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Location-based online identification of objects in the centre of visual attention using eye tracking}},
  doi          = {10.2390/biecoll-saga2013_10},
  year         = {2013},
}

@inproceedings{35,
  abstract     = {In a typical grocery-shopping trip consumers are overwhelmed not only by the number of products and brands in the store, but also by other possible distractions like advertisements, other consumers or smartphones. In this environment, attention is the key source for investigating the decision processes of customers. Recent mobile eyetracking systems have opened the gate to a better understanding of instore attention. We present perspectives from the two disciplines marketing research and human-computer interaction and refine methodical and technological requirements for attention analysis at the point-of-sale (POS).},
  author       = {Meißner, Martin and Pfeiffer, Jella and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction
Mobile Cognitive Assistance Systems},
  location     = {Bielefeld},
  pages        = {10--13},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Mobile eyetracking for decision analysis at the point-of-sale: Requirements from the perspectives of marketing research and human-computer interaction}},
  doi          = {10.2390/biecoll-saga2013_3},
  year         = {2013},
}

@inproceedings{36,
  author       = {Essig, Kai and Pfeiffer, Thies and Maycock, Jonathan and Schack, Thomas},
  booktitle    = {ISSP 13th World Congress of Sport Psychology - Harmony and Excellence in Sport and Life},
  editor       = {Chi, J.},
  location     = {Bejing Sports University, Bejing, China},
  pages        = {43--46},
  title        = {{Attentive Systems: Modern Analysis Techniques for Gaze Movements in Sport Science}},
  year         = {2013},
}

@inproceedings{37,
  abstract     = {If robots are to successfully interact in a space shared with humans, they should learn the communicative signals humans use in face-to-face interactions. For example, a robot can consider human presence for grasping decisions using a representation of peripersonal space (Holthaus \&Wachsmuth, 2012). During interaction, the eye gaze of the interlocutor plays an important role. Using mechanisms of joint attention, gaze can be used to ground objects during interaction and knowledge about the current goals of the interlocutor are revealed (Imai et al., 2003). Eye movements are also known to precede hand pointing or grasping (Prablanc et al., 1979), which could help robots to predict areas with human activities, e.g. for security reasons. We aim to study patterns of gaze and pointing in interaction space. The human participants’ task is to jointly plan routes on a floor plan. For analysis, it is necessary to find fixations on specific rooms and floors as well as on the interlocutor’s face or hands. Therefore, a model-based approach for automating this mapping was developed. This approach was evaluated using a highly accurate outside-in tracking system as baseline and a newly developed low-cost inside-out marker-based tracking system making use of the eye tracker’s scene camera.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction},
  location     = {Bielefeld},
  pages        = {28--31},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Studying joint attention and hand-eye coordination in human-human interaction: A model-based approach to an automatic mapping of fixations to target objects}},
  doi          = {10.2390/biecoll-saga2013_7},
  year         = {2013},
}

@inproceedings{38,
  abstract     = {In this paper we evaluate spatial presence and orientation in the OCTAVIS system, a novel virtual reality platform aimed at training and rehabilitation of visual-spatial cognitive abilities. It consists of eight touch-screen displays surrounding the user, thereby providing a 360 horizontal panorama view. A rotating office chair and a joystick in the armrest serve as input devices to easily navigate through the virtual environment. We conducted a two-step experiment to investigate spatial orientation capabilities with our device. First, we examined whether the extension of the horizontal field of view from 135 (three displays) to 360 (eight displays) has an effect on spatial presence and on the accuracy in a pointing task. Second, driving the full eight screens, we explored the effect of embodied self-rotation using the same measures. In particular we compare navigation by rotating the world while the user is sitting stable to a stable world and a self-rotating user.},
  author       = {Dyck, Eugen and Pfeiffer, Thies and Botsch, Mario},
  booktitle    = {Joint Virtual Reality Conference of EGVE - EuroVR},
  editor       = {Mohler, Betty and Raffin, Bruno and Saito, Hideo and Staadt, Oliver},
  keyword      = {Virtual Reality},
  pages        = {1--8},
  publisher    = {Eurographics Association},
  title        = {{Evaluation of Surround-View and Self-Rotation in the OCTAVIS VR-System}},
  doi          = {10.2312/EGVE.JVRC13.001-008},
  year         = {2013},
}

@article{39,
  abstract     = {Unsere Augen sind für die Wahrnehmung unserer Umwelt wichtig und geben gleichzeitig wertvolle Informationen über unsere Aufmerksamkeit und damit unsere Denkprozesse preis. Wir Menschen nutzen dies ganz natürlich in der alltäglichen Kommunikation. Mit einer echtzeitfähigen Blickbewegungsmessung ausgestattet können auch technische Systeme den Nutzern wichtige Informationen von den Augen ablesen. Der Artikel beschreibt verschiedene Ansätze wie in der Konstruktion, der Instruktion von Robotern oder der Medizin Blickbewegungen nutzbringend eingesetzt werden können. 
/
We use our eyes to perceive our everyday environment. In doing so, we also reveal our current focus of attention and thus allow others to draw insights regarding our internal cognition processes. We humans make use of this dual function of the eyes quite naturally in everyday communication. Using realtime eye tracking, technical systems can learn to read relevant information from their users' eyes. This article describes approaches to make use of gaze information in construction tasks, the instruction of robots or in medical applications.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  journal      = {at - Automatisierungstechnik},
  keyword      = {Visual Attention, Multimodal Interaction, Human-Machine Interaction, Eye Tracking, Attentive Interfaces, Visuelle Aufmerksamkeit, Multimodale Interaktion, Mensch-Maschine-Interaktion, Aufmerksame Benutzerschnittstellen, Blickbewegungsmessung, Gaze-based Interaction},
  number       = {11},
  pages        = {770--776},
  publisher    = {Walter de Gruyter GmbH},
  title        = {{Multimodale blickbasierte Interaktion}},
  doi          = {10.1524/auto.2013.1058},
  volume       = {61},
  year         = {2013},
}

@inproceedings{40,
  abstract     = {Das Paper arbeitet den Forschungsstand zur Überwindung von Höhenunterschieden in der Virtuellen Realität (VR) auf und diskutiert insbesondere deren Einsatz in egozentrischer Perspektive. Am konkreten Beispiel einer VR-Version des Computerspiels Minecraft wird herausgestellt, dass bestehende Ansätze den Anforderungen dieser Anwendungen nicht genügen.},
  author       = {Orlikowski, Matthias and Bongartz, Richard and Reddersen, Andrea and Reuter, Jana and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 10. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Staadt, Oliver and Steinicke, Frank},
  keyword      = {Virtual Reality, Human-Computer Interaction},
  pages        = {193--196},
  publisher    = {Shaker Verlag},
  title        = {{Springen in der Virtuellen Realität: Analyse von Navigationsformen zur Überwindung von Höhenunterschieden am Beispiel von MinecraftVR}},
  year         = {2013},
}

@inproceedings{41,
  abstract     = {Die synthetische Stimulation der visuellen Wahrnehmung ist seit jeher im Fokus von Virtueller und Erweiterter Realität und die möglichst exakte Bestimmung der Nutzerperspektive auf die dreidimensionale Welt eine der Kernaufgaben. Bislang gibt es jedoch nur einige exemplarische Ansätze, in denen die Blickrichtung des Nutzers oder gar die Verteilung der visuellen Aufmerksamkeit im Raum genauer bestimmt wird. Macht man diese Informationen der Anwendungslogik verfügbar, könnten existierende Verfahren zur Visualisierung weiter optimiert und neue Verfahren entwickelt werden. Darüber hinaus erschließen sich damit Blicke als Interaktionsmodalität. Aufbauend auf langjährigen Erfahrungen mit der Blickinteraktion in der Virtuellen Realität beschreibt der Artikel Komponenten für einen Szenengraph, mit dem sich blickbasierte Interaktionen leicht und entlang gewohnter Prinzipien realisieren lassen.},
  author       = {Pfeiffer, Thies},
  booktitle    = {11. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Virtual Reality, Human-Machine Interaction, Visual Attention, Gaze-based Interaction},
  pages        = {295--307},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Visuelle Aufmerksamkeit in Virtueller und Erweiterter Realität: Integration und Nutzung im Szenengraphen}},
  volume       = {311},
  year         = {2013},
}

@inbook{42,
  abstract     = {Human hand gestures are very swift and difficult to observe from the (often) distant perspective of a scientific overhearer. Not uncommonly, fingers are occluded by other body parts or context objects and the true hand posture is often only revealed to the addressee. In addition to that, as the hand has many degrees of freedom and the annotation has to cover positions and orientations in a 3D world – which is less accessible from the typical computer-desktop workplace of an annotator than, let’s say, spoken language – the annotation of hand postures is quite expensive and complex.

Fortunately, the research on virtual reality technology has brought about data gloves in the first place, which were meant as an interaction device allowing humans to manipulate entities in a virtual world. Since its release, however, many different applications have been found. Data gloves are devices that track most of the joints of the human hand and generate data-sets describing the posture of the hand several times a second. The article reviews different types of data gloves, discusses representation formats and ways to support annotation, and presents best practices for study design using data gloves as recording devices.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Body-Language-Communication: An International Handbook on Multimodality in Human Interaction},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Multimodal Corpora, Motion Capturing, Data Gloves},
  pages        = {868--869},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with data gloves}},
  doi          = {10.1515/9783110261318.868},
  volume       = {38/1},
  year         = {2013},
}

@inbook{43,
  abstract     = {For the scientific observation of non-verbal communication behavior, video recordings are the state of the art. However, everyone who has conducted at least one video-based
study has probably made the experience, that it is difficult to get the setup right, with respect to image resolution, illumination, perspective, occlusions, etc. And even more effort is needed for the annotation of the data. Frequently, even short interaction sequences may consume weeks or even months of rigorous full-time annotations.
One way to overcome some of these issues is the use of motion capturing for assessing (not only) communicative body movements. There are several competing tracking technologies available, each with its own benefits and drawbacks. The article provides an overview of the basic types of tracking systems, presents representation formats and tools for the analysis of motion data, provides pointers to some studies using motion capture and discusses best practices for study design. However, the article also stresses that motion capturing still requires some expertise and is only starting to become mobile
and reasonably priced – arguments not to be neglected.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Motion Capturing, Gesture Annotation, Multimodal Corpora},
  pages        = {857--868},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with motion capture}},
  doi          = {10.1515/9783110261318.857},
  volume       = {38/1},
  year         = {2013},
}

@misc{44,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  editor       = {Holmqvist, Kenneth and Mulvey, F. and Johansson, Roger},
  keyword      = {Joint Attention},
  location     = {Lund, Sweden},
  number       = {3},
  pages        = {152--152},
  publisher    = {Journal of Eye Movement Research},
  title        = {{A model of joint attention for humans and machines}},
  volume       = {6},
  year         = {2013},
}

@inproceedings{45,
  abstract     = {A fundamental problem in manual based gesture semantics reconstruction is the specification of preferred semantic concepts for gesture trajectories. This issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories. Based on a detailed example of a gesticulated circular trajectory,
we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing
(mocap) technology. In our FA3ME framework we use a complex event processing approach to analyse and annotate multi-modal events. This framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data.},
  author       = {Pfeiffer, Thies and Hofmann, Florian and Hahn, Florian and Rieser, Hannes and Röpke, Insa},
  booktitle    = {Proceedings of the Special Interest Group on Discourse and Dialog (SIGDIAL) 2013 Conference},
  editor       = {Eskenazi, Maxine and Strube, Michael and Di Eugenio, Barbara and Williams, Jason D.},
  keyword      = {Multimodal Communication},
  location     = {Metz, France},
  pages        = {270--279},
  publisher    = {Association for Computational Linguistics},
  title        = {{Gesture semantics reconstruction based on motion capturing and complex event processing: a circular shape example}},
  year         = {2013},
}

@inproceedings{46,
  abstract     = {This paper presents ongoing work on the design, deployment and evaluation of a multimodal data acquisition architecture
which utilises minimally invasive motion, head, eye and gaze tracking alongside high-quality audiovisual recording of
human interactions. The different data streams are centrally collected and visualised at a single point and in real time by
means of integration in a virtual reality (VR) environment. The overall aim of this endeavour is the implementation of a
multimodal data acquisition facility for the purpose of studying non-verbal phenomena such as feedback gestures,
hand and pointing gestures and multi-modal alignment. In the first part of this work that is described here, a series of tests
were performed in order to evaluate the feasibility of tracking feedback head gestures using the proposed architecture.},
  author       = {Kousidis, Spyridon and Pfeiffer, Thies and Malisz, Zofia and Wagner, Petra and Schlangen, David},
  booktitle    = {Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog, INTERSPEECH2012 Satellite Workshop},
  keyword      = {Multimodal Communication},
  location     = {Stevenson, WA},
  pages        = {39--42},
  title        = {{Evaluating a minimally invasive laboratory architecture for recording multimodal conversational data.}},
  year         = {2012},
}

@misc{47,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of KogWis 2012},
  editor       = {Dörner, Dietrich and Goebel, Rainer and Oaksford, Mike and Pauen, Michael and Stern, Elsbeth},
  keyword      = {gaze-based interaction, cognitive modeling, joint attention},
  location     = {Bamberg, Germany},
  pages        = {96--97},
  publisher    = {University of Bamberg Press},
  title        = {{An operational model of joint attention - Timing of the initiate-act in interactions with a virtual human}},
  year         = {2012},
}

@inproceedings{48,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
  editor       = {Miyake, Naomi  and Peebles, David  and Cooper, Richard P. },
  keyword      = {joint attention, Gaze-based Interaction, social interaction, virtual humans},
  location     = {Sapporo, Japan},
  pages        = {851--856},
  publisher    = {Cognitive Science Society},
  title        = {{An operational model of joint attention - timing of gaze patterns in interactions between humans and a virtual human}},
  year         = {2012},
}

@inproceedings{49,
  abstract     = {Eye tracking and hand motion (or mouse) tracking are complementary techniques to study the dynamics underlying
human cognition. Eye tracking provides information about attention, reasoning, mental imagery, but figuring out the dynamics of cognition is hard. On the other hand, hand movement reveals the hidden states of high-level cognition as a continuous trajectory, but the detailed process is difficult to infer. Here, we use both eye and hand tracking while the subject watches a video drama and plays a multimodal memory game (MMG), a memory recall task designed to investigate the mechanism of recalling the contents of dramas. Our experimental results show that eye tracking and mouse tacking provide complementary information on cognitive processes. In particular, we found that, when humans make difficult decisions, they tend to ask ’Is the
distractor wrong?’, rather than ’Is the decision right?’.},
  author       = {Kim, Eun-Sol and Kim, Jiseob and Pfeiffer, Thies and Wachsmuth, Ipke and Zhang, Byoung-Tak},
  booktitle    = {Proceedings of the 34th Annual Meeting of the Cognitive Science Society},
  editor       = {Miyake, Naomi and Peebles, David and Cooper, Richard P.},
  keyword      = {Gaze-based Interaction},
  pages        = {2723},
  title        = {{‘Is this right?’ or ‘Is that wrong?’: Evidence from dynamic eye-hand movement in decision making [Abstract]}},
  year         = {2012},
}

@inproceedings{50,
  abstract     = {In this paper, we argue that empirical research on genuine linguistic topics, such as on the production of multimodal utterances in the speaker and the interpretation of the multimodal signals in the interlocutor, can greatly benefit from the use of virtual reality technologies. Established methodologies for research on multimodal interactions, such as the presentation of pre-recorded 2D videos of interaction partners as stimuli and the recording of interaction partners using multiple 2D video cameras have crucial shortcomings regarding ecological validity and the precision of measurements that can be achieved. In addition, these methodologies enforce restrictions on the researcher. The stimuli, for example, are not very interactive and thus not as close to natural interactions as ultimately desired. Also, the analysis of 2D video recordings requires intensive manual annotations, often frame-by-frame, which negatively affects the feasible number of interactions which can be included in a study. The technologies bundled under the term virtual reality offer exciting possibilities for the linguistic researcher: gestures can be tracked without being restricted to fixed perspectives, annotation can be done on large corpora (semi-)automatically and virtual characters can be used to produce specific linguistic stimuli in a repetitive but interactive fashion. Moreover, immersive 3D visualizations can be used to recreate a simulation of the recorded interactions by fusing the raw data with theoretic models to support an iterative data-driven development of linguistic theories. This paper discusses the potential of virtual reality technologies for linguistic research and provides examples for the application of the methodology.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtual Reality Short Papers and Posters (VRW)},
  editor       = {Coquillart, Sabine and Feiner, Steven and Kiyokawa, Kiyoshi},
  keyword      = {Linguistics, Motion Capturing, Intelligent Virtual Agents, Multimodal Communication, Virtual Reality},
  location     = {Costa Mesa, CA, USA},
  pages        = {83--84},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  title        = {{Using virtual reality technology in linguistic research}},
  doi          = {10.1109/VR.2012.6180893},
  year         = {2012},
}

@inproceedings{51,
  abstract     = {Modern computer-algebra programs are able to solve a wide
range of mathematical calculations. However, they are not able to understand and solve math text problems in which the equation is described in terms of natural language instead of mathematical formulas. Interestingly, there are only few known approaches to solve math word problems algorithmically and most of employ models based on frames. To overcome problems with existing models, we propose a model based on augmented semantic networks to represent the mathematical structure behind word problems. This model is implemented in our Solver for Mathematical Text Problems (SoMaTePs) [1], where the math problem is extracted via natural language processing, transformed in mathematical equations and solved by a state-of-the-art computer-algebra program. SoMaTePs is able to understand and solve mathematical text problems from German primary school books and could be extended to other languages by exchanging the language model in the natural language processing module.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {Natural Language Processing and Information Systems/17th International Conference on Applications of Natural Language to Information Systems},
  editor       = {Bouma, Gosse and Ittoo, Ashwin and Métais, Elisabeth and Wortmann, Hans},
  keyword      = {Artificial Intelligence},
  location     = {Groningen, Netherlands},
  pages        = {247--252},
  publisher    = {Springer},
  title        = {{Modeling math word problems with augmented semantic networks}},
  doi          = {10.1007/978-3-642-31178-9_29},
  volume       = {7337},
  year         = {2012},
}

@inproceedings{52,
  abstract     = {Knowledge about the point of regard is a major key for the analysis of visual attention in areas such as psycholinguistics, psychology, neurobiology, computer science and human factors. Eye tracking is thus an established methodology in these areas, e.g. for investigating search processes, human communication behavior, product design or human-computer interaction. As eye tracking is a process which depends heavily on technology, the progress of gaze use in these scientific areas is tied to the advancements of eye-tracking technology. It is thus not surprising that in the last decades, research was primarily based on 2D stimuli and rather static scenarios, regarding both content and observer. Only with the advancements in mobile and robust eye-tracking systems, the observer is freed to physically interact in a 3D target scenario. Measuring and analyzing the point of regards in 3D space, however, requires additional techniques for data acquisition and scientific visualization. We describe the process for measuring the 3D point of regard and provide our own implementation of this process, which extends recent approaches of combining eye tracking with motion capturing, including holistic estimations of the 3D point of regard. In addition, we present a refined version of 3D attention volumes for representing and visualizing attention in 3D space.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  editor       = {Spencer, Stephen N.},
  keyword      = {gaze tracking, visualization, motion tracking, Gaze-based Interaction, visual attention, 3d},
  location     = {Santa Barbara, CA, USA},
  pages        = {29--36},
  publisher    = {Association for Computing Machinery (ACM)},
  title        = {{Measuring and visualizing attention in space with 3D attention volumes}},
  doi          = {10.1145/2168556.2168560},
  year         = {2012},
}

@inproceedings{53,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how pragmatic factors affect content distribution between the modalities speech and gesture. This is done by analyzing a study on deictic pointing gestures to objects under two conditions: with and without speech. The relevant pragmatic factor was the distance to the referent object. As one main result two strategies were identified which were used by participants to adapt their gestures to the condition. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures and Sign Language in Human-Computer Interaction and Embodied Communication, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios and Fotinea, Stavroula-Evita},
  keyword      = {Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {238--249},
  publisher    = {Springer-Verlag GmbH},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  doi          = {10.1007/978-3-642-34182-3_22},
  year         = {2012},
}

@inproceedings{54,
  abstract     = {The time course and the distribution of visual attention are powerful measures for the evaluation of the usability of products. Eye tracking is thus an established method for evaluating websites, software ergonomy or modern cockpits for cars or airplanes. In most cases, however, the point of regard is measured on 2D products. This article presents work that uses an approach to measure the point of regard in 3D to generate 3D Attention Volumes as a qualitative 3D visualization of the distribution of visual attention. This visualization can be used to evaluate the design of virtual products in an immersive 3D setting, similar as heatmaps are used to assess the design of websites.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Gaze-based Interaction},
  location     = {Orange County, CA, USA},
  pages        = {117--118},
  publisher    = {IEEE},
  title        = {{3D Attention Volumes for Usability Studies in Virtual Reality}},
  year         = {2012},
}

@inproceedings{55,
  abstract     = {Swiftness and robustness of natural communication is tied to the redundancy and complementarity found in our multimodal communication. Swiftness and robustness of human-computer interaction (HCI) is also a key to the success of a virtual reality (VR) environment. The interpretation of multimodal interaction signals has therefore been considered a high goal in VR research, e.g. following the visions of Bolt's put-that-there in 1980. It is our impression that research on user interfaces for VR systems has been focused primarily on finding and evaluating technical solutions and thus followed a technology-oriented approach to HCI. In this article, we argue to complement this by a human-oriented approach based on the observation of human-human interaction. The aim is to find models of human-human interaction that can be used to create user interfaces that feel natural. As the field of Linguistics is dedicated to the observation and modeling of human-human communication, it could be worthwhile to approach natural user interfaces from a linguistic perspective. We expect at least two benefits from following this approach. First, the human-oriented approach substantiates our understanding of natural human interactions. Second, it brings about a new perspective by taking the interaction capabilities of a human addressee into account, which are not often explicitly considered or compared with that of the system. As a consequence of following both approaches to create user interfaces, we expect more general models of human interaction to emerge.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Linguistics, Virtual Reality, Human-Computer Interaction, Deixis, Multimodal Communication},
  location     = {Orange County, CA, USA},
  pages        = {89--90},
  publisher    = {IEEE},
  title        = {{Towards a Linguistically Motivated Model for Selection in Virtual Reality}},
  year         = {2012},
}

@inbook{56,
  author       = {Lücking, Andy and Pfeiffer, Thies},
  booktitle    = {Handbook of Technical Communication},
  editor       = {Mehler, Alexander and Romary, Laurent},
  keyword      = {Multimodal Communication},
  pages        = {591--644},
  publisher    = {Mouton de Gruyter},
  title        = {{Framing Multimodal Technical Communication. With Focal Points in Speech-Gesture-Integration and Gaze Recognition}},
  doi          = {10.1515/9783110224948.591},
  volume       = {8},
  year         = {2012},
}

@inproceedings{57,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Hülsmann, Felix and Dankert, Timo and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina},
  keyword      = {Gaze-based Interaction},
  pages        = {1--12},
  publisher    = {Shaker Verlag},
  title        = {{Comparing gaze-based and manual interaction in a fast-paced gaming task in Virtual Reality}},
  year         = {2011},
}

@inproceedings{58,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Renner, Patrick and Lüdike, Nico and Wittrowski, Jens and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina },
  keyword      = {Virtual Reality, Gaze-based Interaction},
  pages        = {13--24},
  publisher    = {Shaker Verlag},
  title        = {{Towards Continuous Gaze-Based Interaction in 3D Environments - Unobtrusive Calibration and Accuracy Monitoring}},
  year         = {2011},
}

@misc{59,
  author       = {Essig, Kai and Pfeiffer, Thies and Sand, Norbert and Künsemöller, Jörn and Ritter, Helge and Schack, Thomas},
  booktitle    = {Journal of Eye Movement Research},
  keyword      = {annotation, object recognition, computer vision, Gaze-based Interaction, eye-tracking},
  location     = {Marseille},
  number       = {3},
  pages        = {48},
  title        = {{JVideoGazer - Towards an Automatic Annotation of Gaze Videos from Natural Scenes}},
  volume       = {4},
  year         = {2011},
}

@article{60,
  abstract     = {Social networking platforms (SNPs) are meant to reflect the social relationships of their users. Users typically enter very personal information and should get useful feedback about their social network. They should indeed be empowered to exploit the information they have entered. In reality, however, most SNPs actually hide the structure of the user’s rich social network behind very restricted text-based user interfaces and large parts of the potential information which could be extracted from the entered data lies fallow. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of how to deploy interactive 3D graphical interfaces to large user communities. This is demonstrated on the social graph app-
lication FriendGraph3D for Facebook.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  journal      = {International Journal of Computer Information Systems and Industrial Management Applications},
  keyword      = {information visualization, interactive graphs, social networks, web technology, 3d graphics},
  pages        = {427--434},
  title        = {{Interactive 3D graphs for web-based social networking platforms}},
  volume       = {3},
  year         = {2011},
}

@inproceedings{61,
  abstract     = {Since 2004 the virtual agent Max is living at the Heinz Nixdorf MuseumsForum – a computer science museum. He is welcoming and entertaining visitors ten hours a day, six days a week, for seven years. This article brings together the experiences made by the staff of the museum, the scientists who created and maintained the installation, the visitors and the agent himself. It provides insights about the installation’s hard- and software and presents highlights of the agent’s ontogenesis in terms of the features he has gained. A special focus is on the means Max uses to engage with visitors and the features which make him attractive.},
  author       = {Pfeiffer, Thies and Liguda, Christian and Wachsmuth, Ipke and Stein, Stefan},
  booktitle    = {Proceedings of the Re-Thinking Technology in Museums 2011 - Emerging Experiences},
  editor       = {Barbieri , Sara  and Scott, Katherine  and Ciolfi, Luigina},
  keyword      = {Embodied Conversational Agent, ECA, Chatterbot, Max, Museum, Artificial Intelligence, Virtual Agent},
  location     = {Limerick},
  pages        = {121--131},
  publisher    = {thinkk creative \& the University of Limerick},
  title        = {{Living with a Virtual Agent: Seven Years with an Embodied Conversational Agent at the Heinz Nixdorf MuseumsForum}},
  year         = {2011},
}

@inproceedings{62,
  abstract     = {Die Messung visueller Aufmerksamkeit mittels Eye-Tracking ist eine etablierte Methode in der Bewertung von Ergonomie und Usability. Ihr Gegenstandsbereich beschränkt sich jedoch primär auf 2D-Inhalte wie Webseiten, Produktfotos oder –videos. Bewegte Interaktion im dreidimensionalen Raum wird selten erfasst, weder am realen Objekt, noch am virtuellen Prototyp. Mit einer Aufmerksamkeitsmessung im Raum könnte der Gegenstandsbereich um diese Fälle deutlich erweitert werden. Der vorliegende Artikel arbeitet den aktuellen Stand der Forschung zur Messung visueller Aufmerksamkeit im Raum auf. Dabei werden insbesondere die zu bewältigenden Schwierigkeiten herausgearbeitet und Lösungsansätze aufgezeigt. Als Schwerpunkt werden drei Themen an eigenen Arbeiten diskutiert: Aufbau und Kalibrierung der Systeme, Bestimmung des betrachteten Volumens und Visualisierung der Aufmerksamkeit im Raum.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {10. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Gaze-based Interaction},
  number       = {295},
  pages        = {39--51},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Dreidimensionale Erfassung visueller Aufmerksamkeit für Usability-Bewertungen an virtuellen Prototypen}},
  year         = {2011},
}

@book{63,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and – normally – they will understand. If only machines could do alike.

The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural. 

Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.

To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions: First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction.

In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for Deictic Reference In Virtual Environments (DRIVE). DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check-backs, shared attention and resolving deictic reference. 

Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling.

As empirical basis for the simulations, we created a substantial corpus with highprecision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.

Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling – which we follow in this thesis – requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer (IADE). IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data
and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended Mc-Neill’s 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Multimodal Communication, Gaze-based Interaction},
  pages        = {217},
  publisher    = {Shaker Verlag},
  title        = {{Understanding Multimodal Deixis with Gaze and Gesture in Conversational Interfaces}},
  year         = {2011},
}

@inproceedings{64,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how content is distributed between the modalities speech and gesture by comparing deictic pointing gestures to objects with and without speech. As a result, two main strategies used by participants to adapt their gestures to the condition were identified. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures in Embodied Communication and Human-Computer Interaction, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios},
  keyword      = {Interaction between Speech and Gesture, Gesture, Speech, Pointing, Multimodal Fusion, Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {109--112},
  publisher    = {National and Kapodistrian University of Athens},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  year         = {2011},
}

@inproceedings{65,
  abstract     = {Solving word problems is an important part in school education in primary as well as in high school. Although, the equations that are given by a word problem could be solved by most computer algebra programs without problems, there are just few systems that are able to solve word problems. In this paper we present the ongoing work on a system, that is able to solve word problems from german primary school math books.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {First International Workshop on Algorithmic Intelligence},
  editor       = {Messerschmidt, Hartmut},
  keyword      = {Artificial Intelligence, Natural Language Processing, Math Word Problems},
  location     = {Berlin},
  title        = {{A Question Answer System for Math Word Problems}},
  year         = {2011},
}

@inproceedings{66,
  abstract     = {Classic techniques for navigation and selection such as Image-Plane and World in Miniature have been around for more than 20 years. In the course of a seminar on interaction in virtual reality we reconsidered five methods for navigation and two for selection. These methods were significantly extended by the use of up-to-date hardware such as Fingertracking devices and the Nintendo Wii Balance Board and evaluated in a virtual supermarket scenario. Two user studies, one on experts and one on novices, revealed information on usability and efficiency. As an outcome, the combination of Ray-Casting and Walking in Place turned out to be the fastest.},
  author       = {Renner, Patrick and Dankert, Timo and Schneider, Dorothe and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realitaet: 7. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {human-machine-interaction
},
  location     = {Stuttgart},
  pages        = {71--82},
  publisher    = {Shaker Verlag},
  title        = {{Navigating and selecting in the virtual supermarket: review and update of classic interaction techniques}},
  year         = {2010},
}

@inproceedings{67,
  abstract     = {Humans perceive, reason and act within a 3D environment. In empirical methods, however, researchers often restrict themselves to 2D, either in using 2D content or relying on 2D recordings for analysis, such as videos or 2D eye movements. Regarding, e.g., multimodal deixis, we address the open question of the morphology of the referential space (Butterworth and Itakura, 2000), For modeling the referential space of gaze pointing, precise knowledge about the target of our participants’ visual attention is crucial. To this ends, we developed methods to assess the location of the point of regard, which are outlined here.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Gaze-based Interaction},
  location     = {Potsdam, Germany},
  pages        = {220--221},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Tracking and Visualizing Visual Attention in Real 3D Space}},
  year         = {2010},
}

@inproceedings{68,
  abstract     = {From the perspective of individual users, social networking platforms (SNPs) are meant to reflect their social relationships. SNPs should provide feedback allowing users to exploit the information they have entered. In reality, however, most SNPs actually hide the rich social network constructed by the users in their databases behind simple user interfaces. These interfaces reduce the complexity of a user's social network to a text-based list in HTML. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of deployment of rich interfaces. A social graph application for Facebook is presented, demonstrating how WebGL and HTML5/X3D can be used to implement rich social applications based on upcoming web standards.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Proceedings of the IADIS International Conference Web Virtual Reality and Three-Dimensional Worlds 2010},
  keyword      = {social networks},
  pages        = {269--276},
  publisher    = {IADIS Press},
  title        = {{Relationships in social networks revealed: a facebook app for social graphs in 3D based on X3DOM and WebGL}},
  year         = {2010},
}

@phdthesis{69,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and -- normally -- they will understand. If only machines could do alike.
The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural.
Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.
To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions:
First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction. In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for DRIVE. DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check- backs, shared attention and resolving deictic reference.
Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling. As empirical basis for the simulations, we created a substantial corpus with high-precision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.
Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling -- which we follow in this thesis -- requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer. IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended McNeill's 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Reference, Gesture, Deixis, Human-Computer Interaction, Mensch-Maschine-Schnittstelle, Lokale Deixis, Blickbewegung, Gaze, Virtuelle Realität, Multimodales System, Referenz <Linguistik>, Gestik, Multimodal Communication, Gaze-based Interaction},
  pages        = {241},
  publisher    = {Universitätsbibliothek},
  title        = {{Understanding multimodal deixis with gaze and gesture in conversational interfaces}},
  year         = {2010},
}

@inproceedings{70,
  abstract     = {Object deixis is at the core of language and an ideal example of multimodality. Speech, gaze and manual gestures are used by interlocutors to refer to objects in their 3D environment. The interplay of verbal expressions and gestures during deixis is an active research topic in linguistics as well as in human-computer interaction. Previously, we conducted a study on manual pointing during dialogue games using state-of-the art tracking technologies to record gestures with high spatial precision (Kranstedt, Lücking, Pfeiffer, Rieser and Wachsmuth, 2006), To reveal strategies in manual pointing gestures, we present an analysis of this data with a new visualization technique.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Multimodal Communication},
  location     = {Potsdam},
  pages        = {221--222},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Object Deixis: Interaction Between Verbal Expressions and Manual Pointing Gestures}},
  year         = {2010},
}

@inproceedings{71,
  abstract     = {Die Portale für soziale Netzwerke im Internet gehen mittlerweile deutlich über die Verwaltung einfacher Bekanntschaftsbeziehungen hinaus. Ihnen liegen immer reichhaltigere Datenmodelle zu Grunde. Darstellung und Exploration dieser Netzwerke sind eine grosse Herausforderung für die Entwickler, wenn beides nicht zu einer solchen für die Benutzer werden soll. Im Rahmen eines studentischen Projektes wurde die dritte Dimension für die Darstellung des komplexen sozialen Netzwerkes von Last.fm nutzbar gemacht. Durch die entwickelte Anwendung SoNAR wird das Netzwerk interaktiv und intuitiv sowohl am Desktop, als auch in der Immersion einer dreiseitigen CAVE explorierbar. Unterschiedliche Relationen des sozialen Graphen können parallel exploriert und damit Zusammenhänge zwischen Individuen intuitiv erfahren werden. Eine Suchfunktion erlaubt dabei die fexible Komposition verschiedener Startknoten für die Exploration.},
  author       = {Bluhm, Andreas and Eickmeyer, Jens and Feith, Tobias and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität: 6. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {social networks},
  pages        = {269--280},
  publisher    = {Shaker Verlag},
  title        = {{Exploration von sozialen Netzwerken im 3D Raum am Beispiel von SONAR für Last.fm}},
  year         = {2009},
}

@inproceedings{72,
  abstract     = {The "Where?" is quite important for Mixed Reality applications: Where is the user looking at? Where should augmentations be displayed? The location of the overt visual attention of the user can be used both to disambiguate referent objects and to inform an intelligent view management of the user interface. While the vertical and horizontal orientation of attention is quite commonly used, e.g. derived from the orientation of the head, only knowledge about the distance allows for an intrinsic measurement of the location of the attention. This contribution reviews our latest results on detecting the location of attention in 3D space using binocular eye tracking.},
  author       = {Pfeiffer, Thies and Mattar, Nikita},
  booktitle    = {Workshop-Proceedings der Tagung Mensch \& Computer 2009: Grenzenlos frei!?},
  keyword      = {Gaze-based Interaction},
  publisher    = {Logos Berlin},
  title        = {{Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications}},
  year         = {2009},
}

@inproceedings{73,
  abstract     = {The Semantic Web is about to become a rich source of knowledge whose potential will be squandered if it is not accessible for everyone. Intuitive interfaces like conversational agents are needed to better disseminate this knowledge, either on request or even proactively in a context-aware manner. This paper presents work on extending an existing conversational agent, Max, with abilities to access the Semantic Web via natural language communication.},
  author       = {Breuing, Alexa and Pfeiffer, Thies and Kopp, Stefan},
  booktitle    = {Proceedings of the Poster and Demonstration Session at the 7th International Semantic Web Conference (ISWC 2008)},
  editor       = {Bizer, Christian and Joshi, Anupam},
  title        = {{Conversational Interface Agents for the Semantic Web - a Case Study}},
  year         = {2008},
}

@inproceedings{74,
  abstract     = {Interaction in conversational interfaces strongly relies on the sys- tem's capability to interpret the user's references to objects via de- ictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Vir- tual Reality.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the IEEE VR 2008},
  editor       = {Lin, Ming and Steed, Anthony and Cruz-Neira, Carolina},
  keyword      = {Multimodal Communication},
  pages        = {281--282},
  publisher    = {IEEE Press},
  title        = {{Conversational pointing gestures for virtual reality interaction: Implications from an empirical study}},
  doi          = {10.1109/vr.2008.4480801},
  year         = {2008},
}

@article{75,
  abstract     = {Tracking user's visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user's visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {JVRB - Journal of Virtual Reality and Broadcasting},
  keyword      = {human-computer interaction, object selection, virtual reality, gaze-based interaction, eye tracking},
  number       = {16},
  pages        = {1660},
  title        = {{Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments}},
  volume       = {5},
  year         = {2008},
}

@inproceedings{76,
  abstract     = {Of all senses, it is visual perception that is predominantly deluded in Virtual Realities. Yet, the eyes of the observer, despite the fact that they are the fastest perceivable moving body part, have gotten relatively little attention as an interaction modality. A solid integration of gaze, however, provides great opportunities for implicit and explicit human-computer interaction. We present our work on integrating a lightweight head-mounted eye tracking system in a CAVE-like Virtual Reality Set-Up and provide promising data from a user study on the achieved accuracy and latency.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - Fünfter Workshop der GI-Fachgruppe VR/AR},
  editor       = {Schumann, Marco and Kuhlen, Torsten},
  keyword      = {Gaze-based Interaction},
  pages        = {81--92},
  publisher    = {Shaker Verlag GmbH},
  title        = {{Towards Gaze Interaction in Immersive Virtual Reality: Evaluation of a Monocular Eye Tracking Set-Up}},
  year         = {2008},
}

@inproceedings{77,
  abstract     = {Emotions and interpersonal distances are identified as key aspects in social interaction. A novel Affective Computer-Mediated Communication (ACMC) framework has been developed making the interplay of both aspects explicit to facilitate social presence. In this ACMC framework, the displays can be arranged in virtual space manually or automatically. We expect that, according to empirical findings, the social relation as well as momentarily affective appraisals will influence this arrangement. The proposed concept extends from desktop devices to fully immersive Virtual Reality interfaces.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 11th International Workshop on Presence},
  editor       = {Spagnolli, Anna and Gamberini, Luciano},
  keyword      = {Mediated Communication},
  pages        = {275--279},
  publisher    = {CLEUP Cooperativa Libraria Universitaria Padova},
  title        = {{Social Presence: The Role of Interpersonal Distances in Affective Computer-Mediated Communication}},
  year         = {2008},
}

@inproceedings{78,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) "Alignment in Communication" a series of experimental investigations of natural Face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Schaffranietz, Gesche and Rickheit, Gert},
  booktitle    = {Cognitive Science 2007: Proceedings of the 8th Annual Conference of the Cognitive Science Society of Germany},
  editor       = {Zimmer, Hubert D. and Frings, C. and Mecklinger, Axel and Opitz, B. and Pospeschill, M. and Wentura, D.},
  keyword      = {Multimodal Communication},
  location     = {Saarbrücken, Germany},
  title        = {{Coordination in dialog: Alignment of object naming in the Jigsaw Map Game}},
  year         = {2008},
}

@misc{79,
  author       = {Meißner, Martin and Essig, Kai and Pfeiffer, Thies and Decker, Reinhold and Ritter, Helge},
  booktitle    = {Perception},
  keyword      = {In a novel approach we investigated choice processes using eye tracking to improve research instruments in marketing research. Choice-based conjoint analysis (CBC) is the most widely-used tool for investigating consumer preferences on the basis of choice tasks. While CBC is highly appreciated for its realism (Haaijer and Wedel, 2007), marketing researchers have highlighted that respondents are easily exposed to the problem of information overload (Green et al, 2001). The question how much information is being processed during choice processes and how preference measurement is affected remains an open research issue. We investigated choice processes using eye tracking in a CBC on-line consumer survey. We showed (i) that the extent to which information is processed is decreasing in later choice tasks, (ii) in how far information overload changes the pattern of eye movements, and (iii) how the difficulty of a choice task influences information processing.},
  pages        = {97--97},
  publisher    = {PION LTD},
  title        = {{Eye-tracking decision behaviour in choice-based conjoint analysis}},
  volume       = {37},
  year         = {2008},
}

@inproceedings{80,
  abstract     = {Für die Mensch-Maschine-Interaktion ist die Erfassung der Aufmerksamkeit des Benutzers von großem Interesse. Für Anwendungen in der Virtuellen Realität (VR) gilt dies insbesondere, nicht zuletzt dann, wenn Virtuelle Agenten als Benutzerschnittstelle eingesetzt werden. Aktuelle Ansätze zur Bestimmung der visuellen Aufmerksamkeit verwenden meist monokulare Eyetracker und interpretieren daher auch nur zweidimensionale bedeutungstragende Blickfixationen relativ zu einer Projektionsebene. Für typische Stereoskopie-basierte VR Anwendungen ist aber eine zusätzliche Berücksichtigung der Fixationstiefe notwendig, um so den Tiefenparameter für die Interaktion nutzbar zu machen, etwa für eine höhere Genauigkeit bei der Objektauswahl (Picking). Das in diesem Beitrag vorgestellte Experiment zeigt, dass bereits mit einem einfacheren binokularen Gerät leichter zwischen sich teilweise verdeckenden Objekten unterschieden werden kann. Trotz des positiven Ergebnisses kann jedoch noch keine uneingeschränkte Verbesserung der Selektionsleistung gezeigt werden. Der Beitrag schließt mit einer Diskussion nächster Schritte mit dem Ziel, die vorgestellte Technik weiter zu verbessern.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Virtuelle und Erweiterte Realität. 4. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Fröhlich, Bernd},
  keyword      = {Mensch-Maschine-Interaktion, Eyetracking, Virtuelle Realität},
  pages        = {113--124},
  publisher    = {Shaker},
  title        = {{Blickfixationstiefe in stereoskopischen VR-Umgebungen: Eine vergleichende Studie}},
  year         = {2007},
}

@article{81,
  abstract     = {Humans perceive and act within a visual world. This has become important even in
disciplines which are prima facie not concerned with vision and eye tracking is now used
in a broad range of domains. However, the world we are in is not two-dimensional, as
many experiments may convince us to believe. It is convenient to use stimuli in 2D or
21/2D and in most cases absolutely appropriate; but it is often technically motivated and
not scientifically.
To overcome these technical limitations we contribute results of an evaluation of different
approaches to calculate the depth of a fixation based on the divergence of the eyes
by testing them on different devices and within real and virtual scenarios.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {Journal of Eye Movement Research},
  keyword      = {Gaze-based Interaction},
  pages        = {13},
  title        = {{3D fixations in real and virtual scenarios}},
  volume       = {Special issue: Abstracts of the ECEM 2007},
  year         = {2007},
}

@inproceedings{82,
  abstract     = {Im Rahmen der Entwicklung einer multimodalen Schnittstelle für die Mensch-Maschine Kommunikation konzentriert sich diese Arbeit auf die Interpretation von Referenzen auf sichtbare Objekte. Im Vordergrund stehen dabei Fragen zur Genauigkeit von Zeigegesten und deren Interaktion mit sprachlichen Ausdrücken. Die Arbeit spannt dabei methodisch einen Bogen von Empirie über Simulation und Visualisierung zur Modellbildung und Evaluation. In Studien zur deiktischen Objektreferenz wurden neben sprachlichen Äußerungen unter dem Einsatz moderner Motion Capturing Technik umfangreiche Daten zum deiktischen Zeigen erhoben. Diese heterogenen Daten, bestehend aus Tracking Daten, sowie Video und Audio Aufzeichnungen, wurden annotiert und mit eigens entwickelten interaktiven Werkzeugen unter Einsatz von Techniken der Virtuellen Realität integriert und aufbereitet. Die statistische Auswertung der Daten erfolgte im Anschluß mittels der freien Statistik-Software R. Die datengetriebene Modellbildung bildet die Grundlage für die Weiterentwicklung eines unscharfen, fuzzy-basierten, Constraint Satisfaction Ansatzes zur Interpretation von Objektreferenzen. Wesentliches Ziel ist dabei eine inkrementelle, echtzeitfähige Verarbeitung, die den Einsatz in direkter Mensch-Maschine Interaktion erlaubt. Die Ergebnisse der Studie haben über die Fragestellung hinaus Einfluss auf ein Modell zur Produktion von deiktischen Ausdrücken und direkte Konsequenzen für einschlägige Theorien zur deiktischen Referenz.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Bertram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {multimodal communication},
  pages        = {109--110},
  publisher    = {Shaker Verlag},
  title        = {{Interpretation von Objektreferenzen in multimodalen Äußerungen}},
  year         = {2007},
}

@inproceedings{83,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) “Alignment in Communication” a series of experimental investigations of natural face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Schaffranietz, Gesche and Weiß, Petra and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Betram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {Multimodal Communication},
  pages        = {41--42},
  publisher    = {Shaker Verlag},
  title        = {{Ein Experiment zur Koordination von Objektbezeichnungen im Dialog}},
  year         = {2007},
}

@inproceedings{84,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich},
  booktitle    = {IPT-EGVE 2007, Virtual Environments 2007, Short Papers and Posters},
  editor       = {Fröhlich, Bernd and Blach, Roland and Liere van, Robert},
  keyword      = {Mediated Communication},
  pages        = {41--42},
  publisher    = {Eurographics Association},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@misc{85,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc E.},
  keyword      = {Mediated Communication},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@inbook{86,
  abstract     = {This chapter presents an original approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we take steps to classify the role of pointing in deictic expressions and to model the focussed area of pointing gestures, the so-called pointing cone. This pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {155--208},
  publisher    = {Mouton de Gruyter},
  title        = {{Deictic object reference in task-oriented dialogue}},
  doi          = {10.1515/9783110197747.155},
  year         = {2006},
}

@inbook{87,
  abstract     = {This contribution presents investigations of the usage of computer gene¬rated 3D stimuli for psycholinguistic experiments. In the first part, we introduce VDesigner. VDesigner is a visual programming environment that operates in two different modes, a design mode to implement the materials and the structure of an experiment, and a runtime mode to actually run the experiment. We have extended VDesigner to support interactive experimentation in 3D. In the second part, we de-scribe a practical application of the programming environment. We have replicated a previous 2½D study of the production of spatial terms in a 3D setting, with the objective of investigating the effect of the presentation modes (2½D vs. 3D) on the choice of the referential system. In each trial, on being presented with a scene, the participants had to verbally specify the position of a target object in relation to a reference object. We recorded the answers of the participants as well as their reac-tion times. The results suggest that stereoscopic 3D presentations are a promising technology to elicit a more natural behavior of participants in computer-based experiments.},
  author       = {Flitter, Helmut and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {127--153},
  publisher    = {Mouton de Gruyter},
  title        = {{Psycholinguistic experiments on spatial relations using stereoscopic presentation}},
  year         = {2006},
}

@inproceedings{88,
  abstract     = {Für die empirische Erforschung situierter natürlicher menschlicher Kommunikation sind wir auf die Akquise und Auswertung umfangreicher Daten angewiesen. Die Modalitäten, über die sich Menschen ausdrücken können, sind sehr unterschiedlich. Entsprechend heterogen sind die Repräsentationen, mit denen die erhobenen Daten für die Auswertung verfügbar gemacht werden können. Für eine Untersuchung des Zeigeverhaltens bei der Referenzierung von Objekten haben wir mit IADE ein Framework für die Aufzeichnung, Analyse und Simulation von Sprach-Gestik Daten entwickelt. Durch den Einsatz von Techniken aus der interaktiven VR erlaubt IADE die synchronisierte Aufnahme von Bewegungs-, Video- und Audiodaten und unterstützt einen iterativen Auswertungsprozess der gewonnenen Daten durch komfortable integrierte Revisualisierungen und Simulationen. Damit stellt IADE einen entscheidenden Fortschritt für unsere linguistische Experimentalmethodik dar.},
  author       = {Pfeiffer, Thies and Kranstedt, Alfred and Lücking, Andy},
  booktitle    = {Dritter Workshop Virtuelle und Erweiterte Realität der GI-Fachgruppe VR/AR},
  editor       = {Müller, Stefan and Zachmann, Gabriel},
  keyword      = {Multimodal Communication},
  pages        = {61--72},
  publisher    = {Shaker},
  title        = {{Sprach-Gestik Experimente mit IADE, dem Interactive Augmented Data Explorer}},
  year         = {2006},
}

@inproceedings{89,
  abstract     = {We present a collaborative approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we attempt to measure the precision of the focussed area of a pointing gesture, the so-called pointing cone. ne pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Gesture in Human-Computer Interaction and Simulation},
  editor       = {Gibet , Sylvie and Courty , Nicolas and Kamp , Jean-François},
  keyword      = {Multimodal Communication},
  pages        = {300--311},
  publisher    = {Springer},
  title        = {{Deixis: How to determine demonstrated objects using a pointing cone}},
  doi          = {10.1007/11678816_34},
  year         = {2006},
}

@inbook{90,
  abstract     = {Instructions play an important role in everyday communication, e.g. in task-oriented dialogs. Based on a (psycho-)linguistic theoretical background, which classifies instructions as requests, we conducted experiments using a cross-modal experimental design in combination with a reaction time paradigm in order to get insights in human instruction processing. We concentrated on the interpretation of basic single sentence instructions. Here, we especially examined the effects of the specificity of verbs, object names, and prepositions in interaction with factors of the visual object context regarding an adequate reference resolution. We were able to show that linguistic semantic and syntactic factors as well as visual context information context influence the interpretation of instructions. Especially the context information proves to be very important. Above and beyond the relevance for basic research, these results are also important for the design of human-computer interfaces capable of understanding natural language. Thus, following the experimental-simulative approach, we also pursued the processing of instructions from the perspective of computer science. Here, a natural language processing interface created for a virtual reality environment served as basis for the simulation of the empirical findings. The comparison of human vs. virtual system performance using a local performance measure for instruction understanding based on fuzzy constraint satisfaction led to further insights concerning the complexity of instruction processing in humans and artificial systems. Using selected examples, we were able to show that the visual context has a comparable influence on the performance of both systems, whereas this approach is limited when it comes to explaining some effects due to variations of the linguistic structure. In order to get deeper insights into the timing and interaction of the sub-processes relevant for instruction understanding and to model these effects in the computer simulation, more specific data on human performance are necessary, e.g. by using eye-tracking techniques. In the long run, such an approach will result in the development of a more natural and cognitively adequate human-computer interface.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Eikmeyer, Hans-Jürgen and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {31--76},
  publisher    = {Mouton de Gruyter},
  title        = {{Processing Instructions}},
  year         = {2006},
}

@inproceedings{91,
  abstract     = {We describe an experiment to gather original data on geometrical aspects of pointing. In particular, we are focusing upon the concept of the pointing cone, a geometrical model of a pointing’s extension. In our setting we employed methodological and technical procedures of a new type to integrate data from annotations as well as from tracker recordings. We combined exact information on position and orientation with rater’s classifications. Our first results seem to challenge classical linguistic and philosophical theories of demonstration in that they advise to separate pointings from reference.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Staudacher, Marc},
  booktitle    = {Proceedings of the brandial 2006 - The 10th Workshop on the Semantics and Pragmatics of Dialogue},
  editor       = {Schlangen, David and Fernández, Raquel},
  keyword      = {Multimodal Communication},
  pages        = {82--89},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Measuring and Reconstructing Pointing in Visual Contexts}},
  year         = {2006},
}

@inproceedings{92,
  abstract     = {Recently videoconferencing has been extended from human face-to-face communication to human machine interaction with Virtual Environments (VE)[6]. Relying on established videoconferencing (VC) protocol standards this thin client solution does not require specialised 3D soft- or hardware and scales well to multimedia enabled mobile devices. This would bring a whole range of new applications to the mobile platform. To facilitate our research in mobile interaction the Open Source project P@CE has been started to bring a fullfeatured VC client to the Pocket PC platform.},
  author       = {Weber, Matthias and Pfeiffer, Thies and Jung, Bernhard},
  booktitle    = {MOBILE HCI 05 Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices and Services},
  editor       = {Tscheligi, Manfred and Bernhaupt, Regina and Mihalic, Kristijan},
  keyword      = {Mediated Communication},
  pages        = {351--352},
  publisher    = {ACM},
  title        = {{Pr@senZ - P@CE: Mobile Interaction with Virtual Reality}},
  year         = {2005},
}

@inproceedings{93,
  abstract     = {This paper presents an alternative to existing methods for remotely accessing Virtual Reality (VR) systems. Common solutions are based on specialised software and/or hardware capable of rendering 3D content, which not only restricts accessibility to specific platforms but also increases the barrier for non expert users. Our approach addresses new audiences by making existing Virtual Environments (VEs) ubiquitously accessible. Its appeal is that a large variety of clients, like desktop PCs and handhelds, are ready to connect to VEs out of the box. We achieve this combining established videoconferencing protocol standards with a server based interaction handling. Currently interaction is based on natural speech, typed textual input and visual feedback, but extensions to support natural gestures are possible and planned. This paper presents the conceptual framework enabling videoconferencing with collaborative VEs as well as an example application for a virtual prototyping system.},
  author       = {Pfeiffer, Thies and Weber, Matthias and Jung, Bernhard},
  booktitle    = {Theory and Practice of Computer Graphics 2005},
  editor       = {Lever, Louise and McDerby, Marc},
  keyword      = {Mediated Communication},
  pages        = {209--216},
  publisher    = {Eurographics Association},
  title        = {{Ubiquitous Virtual Reality: Accessing Shared Virtual Environments through Videoconferencing Technology}},
  year         = {2005},
}

@inproceedings{94,
  abstract     = {This paper describes the underlying concepts and the technical implementation of a system for resolving multimodal references in Virtual Reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a socalled reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.},
  author       = {Pfeiffer, Thies and Latoschik, M. E.},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2004},
  editor       = {Ikei, Yasushi and Göbel, Martin and Chen, Jim},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing ling::Reference Resolution, Multimodal Communication},
  pages        = {35--42},
  title        = {{Resolving Object References in Multimodal Dialogues for Immersive Virtual Environments}},
  year         = {2004},
}

@inproceedings{95,
  abstract     = {This poster presents cognitive-motivated aspects of a technical system for the resolution of references to objects within an assembly-task domain. The research is integrated in the Collaborative Research Center SFB 360 which is concerned with situated artificial communicators. One application scenario consists of a task-oriented discourse between an instructor and a constructor who collaboratively build aggregates from a wooden toy kit (Baufix), or from generic CAD parts. In our current setting this scenario is embedded in a virtual reality (VR) installation, where the human user, taking the role of the instructor, guides the artificial constructor (embodied by the ECA Max) through the assembly process by means of multimodal task descriptions (see Figure 1). The system handles instructions like: Plug the left red screw from above in the middle hole of the wing and turn it this way. accompanied by coverbal deictic and mimetic gestures (see Latoschik, 2001).},
  author       = {Pfeiffer, Thies and Voss, Ian and Latoschik, Marc Erich},
  booktitle    = {Proceedings of the EuroCogSci03},
  editor       = {Schmalhofer, F. and Young, R.},
  keyword      = {inform::Multimodality inform::Virtual Reality ling::Reference Resolution ling::Natural Language Processing, Artificial Intelligence, Multimodal Communication},
  pages        = {426},
  publisher    = {Lawrence Erlbaum Associates Inc},
  title        = {{Resolution of Multimodal Object References using Conceptual Short Term Memory}},
  year         = {2003},
}

@misc{96,
  author       = {Pfeiffer, Thies},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing, Multimodal Communication},
  publisher    = {Faculty of Technology, University of Bielefeld},
  title        = {{Eine Referenzauflösung für die dynamische Anwendung in Konstruktionssituationen in der Virtuellen Realität}},
  year         = {2003},
}

@misc{97,
  abstract     = {When merging different knowledge bases one has to cope with the problem of classifying and linking concepts as well as the possibly heterogeneous representations the knowledge is expressed in. We are presenting an implementation that follows the Model Driven Architecture (MDA) [Miller and Mukerji, 2003] approach defined by the Object Management Group (OMG). Metamodels defined in the Unified Modeling Language (UML) are used to implement different knowledge representation formalisms. Knowledge is expressed as a Model instantiating the Metamodel. Integrating Metamodels are defined for merging knowledge distributed over different knowledge bases.},
  author       = {Pfeiffer, Thies and Voss, Ian},
  keyword      = {Artificial Intelligence},
  title        = {{Integrating Knowledge Bases Using UML Metamodels}},
  year         = {2003},
}

@inproceedings{98,
  abstract     = {This contribution describes a WWW-based multi-user system for concurrent virtual prototyping. A 3D scene of CAD parts is presented to the users in the web browser. By instructing the system using simple natural language commands, complex aggregates can be assembled from the basic parts. The current state of the assembly is instantly published to all system users who can discuss design choices in a chat area. The implementation builds on an existing system for virtual assembly made available as a web service. The client side components are fully implemented as Java applets and require no plugin for visualization of 3D content. Http tunneled messaging between web clients and server ensures system accessibility from any modern web browser even behind firewalls. The system is first to demonstrate natural language based virtual prototyping on the web.},
  author       = {Jung, Bernhard and Pfeiffer, Thies and Zakotnik, Jure},
  booktitle    = {Proceedings Structured Design of Virtual Environments and 3D-Components},
  editor       = {Geiger et al., C.},
  keyword      = {inform::Web inform::Internet inform::Collaborative Environments},
  pages        = {101--110},
  publisher    = {Shaker},
  title        = {{Natural Language Based Virtual Prototyping on the Web}},
  year         = {2002},
}


